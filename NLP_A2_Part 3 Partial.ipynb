{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "with open('/Users/stephendabrowski/Desktop/NLP_Assignment_2/brown_100.txt', 'r') as file_:\n",
    "    corpus = file_.read()\n",
    "\n",
    "def write_freqs(list_: list, filename: str) -> None:\n",
    "    with open(f'./results/{filename}.txt', 'w') as file_:\n",
    "        file_.write('\\n'.join([' '.join(x[0]) + ' | ' + str(x[1]) for x in list_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Modeling\n",
    "In the second assignment, you will implement an **n-gram language model** that processes input text to generate, analyze, and calculate the probabilities of sequences of `n` words (n-grams). This model will tokenize text, create n-grams, and compute their frequencies and probabilities, returning the most frequent n-grams in the text.\n",
    "\n",
    "1. **Tokenization**: Convert the input text into a list of words (tokens).\n",
    "2. **N-gram Generation**: Create sequences of `n` tokens from the text, treating each sentence independently with the inclusion of start (`<s>`) and end (`</s>`) markers.\n",
    "3. **Frequency Counting**: Count how many times each n-gram appears in the text.\n",
    "4. **Probability Calculation**: Compute the probabilities of each n-gram based on its frequency and add alpha smoothing to improve generalization of items.\n",
    "5. **Most Frequent N-grams**: Return the most frequent n-grams along with their probabilities, helping to identify common patterns in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict # You may import more from collections if needed\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, text, n, alpha=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the NGramModel with text and the value of n.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.alpha = alpha  # Alpha value for additive smoothing\n",
    "        self.ngrams = {}\n",
    "        self.probabilities = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        \"\"\"\n",
    "        Tokenize the text into words. \n",
    "        Fill in the code to split the text into a list of words.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        # split on newlines\n",
    "        sentences = self.text.split('\\n')\n",
    "\n",
    "        # split on spaces, add each token to the list\n",
    "        for sentence in sentences:\n",
    "            tokens.extend(token.lower() for token in sentence.split())\n",
    "\n",
    "        # set the vocab to the unique tokens\n",
    "        self.vocab = set(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def generate_ngrams(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Generate n-grams from the list of tokens.\n",
    "        Fill in the code to create n-grams.\n",
    "        Make sure to treat each sentence independently, include the <s> and </s> tokens.\n",
    "\n",
    "        Creates a dictionary that looks like this:\n",
    "        {\n",
    "            ('<s>', 'The'): #,\n",
    "            ('The', 'jury'): #,\n",
    "        }\n",
    "        where in this case n = 2, so the keys are of length 2\n",
    "        \"\"\"\n",
    "        self.ngrams = {}\n",
    "\n",
    "        i = 0\n",
    "        # create and count the ngrams\n",
    "        while i < (len(tokens) - self.n + 1):\n",
    "            # using a window of size n, create an n-gram\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "\n",
    "            # if this n-gram is not in the ngrams[n] dict, add it\n",
    "            if ngram not in self.ngrams:\n",
    "                self.ngrams[ngram] = 0\n",
    "\n",
    "            # add this n-gram to the list of n-grams\n",
    "            self.ngrams[ngram] += 1\n",
    "\n",
    "            # if this n-gram ends with an end tag (</s>), move the window + n so we start a new sentence\n",
    "            if ngram[-1] == '</s>':\n",
    "                i += self.n\n",
    "            # if the n-gram ends with a period and the next token is not an end tag, move the window + n so we start a new sentence\n",
    "            elif ngram[-1] == '.' and tokens[i + self.n] != '</s>':\n",
    "                i += self.n\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return self.ngrams\n",
    "\n",
    "    def count_frequencies(self) -> None:\n",
    "        \"\"\"\n",
    "        Count the frequencies of each n-gram.\n",
    "        Fill in the code to count n-gram occurrences.\n",
    "        \"\"\"\n",
    "        # already counted in generate_ngrams\n",
    "        pass\n",
    "\n",
    "    def calculate_probabilities(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate probabilities of each n-gram based on its frequency. Add alpha smoothing separately.\n",
    "        \"\"\"\n",
    "        self.probabilities = {}\n",
    "        prefix_counts = Counter()\n",
    "        if self.n > 1:\n",
    "            for ngram, count in self.ngrams.items():\n",
    "                prefix_counts[ngram[:-1]] += count\n",
    "        \n",
    "        # Get vocabulary size (unique words)\n",
    "        vocab_size = len(self.vocab)\n",
    "    \n",
    "        # Calculate smoothed probability for each n-gram\n",
    "        for ngram, count in self.ngrams.items():\n",
    "            if self.n > 1:\n",
    "                # get the prefix of the n-gram\n",
    "                prefix = ngram[:-1]\n",
    "\n",
    "                # get the count of the prefix\n",
    "                prefix_count = prefix_counts[prefix]\n",
    "            else:\n",
    "                prefix_count = sum(self.ngrams.values())\n",
    "\n",
    "            # Apply additive (Laplace) smoothing\n",
    "            # if alpha is 0, then implicitly no smoothing is applied\n",
    "            smoothed_prob = (count + self.alpha) / (prefix_count + self.alpha * vocab_size)\n",
    "            self.probabilities[ngram] = smoothed_prob\n",
    "\n",
    "\n",
    "    def most_frequent_ngrams(self, top_n: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Return the most frequent n-grams and their probabilities.\n",
    "        \"\"\"\n",
    "        # Sort n-grams by frequency in descending order\n",
    "        sorted_grams = sorted(self.ngrams.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top_n n-grams with their probabilities\n",
    "        return [(ngram, self.probabilities[ngram]) for ngram, _ in sorted_grams[:top_n]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams\n",
    "\n",
    "In this section, we apply the `NGramModel` to our corpus. The model can be configured to generate unigrams, bigrams, or trigrams by adjusting the value of `n`. After tokenizing the text and generating the n-grams, the model counts the occurrences of each n-gram and calculates their probabilities. The most frequent n-grams are then written to an output file.\n",
    "\n",
    "By adjusting the n-gram size, we can analyze different levels of word dependencies in the text, providing insights into common word sequences and patterns.  \n",
    "\n",
    "The code below will generate unigrams when the NGramModel is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'unigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "Bigrams allow us to explore word pairs and their relationships, providing insight into common word combinations and phrase structures within the corpus.  \n",
    "The code below will generate bigrams when the NGramModel is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams\n",
    "Trigrams capture three-word sequences, providing a deeper understanding of longer word dependencies and commonly occurring phrases within the text.  \n",
    "The code below will generate trigrams when the NGramModel is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "Below, we generate **bigrams** with **smoothing** applied by setting `n = 2` and including a smoothing parameter `alpha = 1.0`. Smoothing is useful in language models to handle unseen n-grams by assigning a small probability to them. The model tokenizes the text, generates bigrams, counts their frequencies, and calculates smoothed probabilities. The most frequent bigrams are then written to an output file.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n, alpha=1.0)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'bigrams_smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "In this example, we generate text using the **bigrams** model by providing a starting **prompt** (e.g., \"the jury\"). The `generate_text` function creates a sequence of words based on the trained bigram model, starting from the provided prompt and continuing based on the probabilities of the next words.\n",
    "\n",
    "After tokenizing the corpus, generating bigrams, and calculating their frequencies and probabilities, the model uses these learned probabilities to generate text. The generated output is then written to a file.\n",
    "\n",
    "Text generation with n-grams helps illustrate how language models can predict word sequences, allowing us to create new sentences that follow similar patterns found in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: NGramModel, n: int, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using the n-gram model starting from the given prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained n-gram model\n",
    "        n: The size of n-grams to use\n",
    "        prompt: The starting text to continue from\n",
    "        \n",
    "    Returns:\n",
    "        A string of generated text\n",
    "    \"\"\"\n",
    "    generated_tokens = prompt.split()\n",
    "    \n",
    "    max_length = len(generated_tokens) + n # Prevent infinite generation\n",
    "    while len(generated_tokens) < max_length:\n",
    "        # Get the last n-1 tokens as context\n",
    "        context = tuple(generated_tokens[-(n-1):]) if n > 1 else tuple()\n",
    "        \n",
    "        # Find all possible next tokens based on the context\n",
    "        possible_next = []\n",
    "        for ngram, prob in model.probabilities.items():\n",
    "            if ngram[:-1] == context:\n",
    "                possible_next.append((ngram[-1], prob))\n",
    "        \n",
    "        if not possible_next:\n",
    "            break  # No possible continuation\n",
    "            \n",
    "        # Choose next token based on probabilities\n",
    "        next_token = max(possible_next, key=lambda x: x[1])[0]\n",
    "\n",
    "        # Add the chosen token\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Stop if we reach a sentence end\n",
    "        if next_token == '</s>':\n",
    "            break\n",
    "            \n",
    "    return ' '.join(generated_tokens)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "generated_text = generate_text(model, 2, 'the jury')\n",
    "with open(f'./results/generated_bigrams.txt', 'w') as file_:\n",
    "    file_.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "Based on the Stanford course: https://web.stanford.edu/class/cs224n/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count based W2V\n",
    "In this section, we use the previously trained **n-gram model** to calculate a **co-occurrence matrix**. A co-occurrence matrix shows how often words appear together within a specific context (in this case, bigrams), providing insights into word relationships in the corpus.\n",
    "\n",
    "Steps:\n",
    "1. **Tokenization and N-gram Generation**: We first tokenize the text and generate bigrams using the `NGramModel`.\n",
    "2. **Vocabulary and Indexing**: The vocabulary is created from the unique tokens, and a word-to-index mapping is established.\n",
    "3. **Co-occurrence Matrix**: Using the generated n-grams, we compute the co-occurrence matrix, where each entry represents how often two words occur together within the bigrams.\n",
    "\n",
    "The co-occurrence matrix provides a numerical representation of word relationships, which is useful for tasks like word embeddings, semantic analysis, and understanding word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Since we already have a working ngram model, for the next part we can use the ngrams to calculate the co-occurences.\n",
    "model = NGramModel(corpus, 2)\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "\n",
    "def create_co_matrix(ngrams: list, vocab_size: int, word_to_index: dict) -> np.ndarray:\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for ngram, count in ngrams.items():\n",
    "        if len(ngram) == 2:  # Only for bigrams\n",
    "            i = word_to_index[ngram[0]]\n",
    "            j = word_to_index[ngram[1]]\n",
    "            co_matrix[i][j] += count\n",
    "    \n",
    "    return co_matrix\n",
    "\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a word-to-index mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Generate the co-occurrence matrix\n",
    "co_matrix = create_co_matrix(ngrams, vocab_size, word_to_index)\n",
    "\n",
    "\n",
    "try:\n",
    "    print(co_matrix)\n",
    "except NameError:\n",
    "    print(\"Run the dimensionality reduction step first!\")\n",
    "\n",
    "# Optionally, print the matrix or parts of it\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below you can verify if the co-occurence works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence count for 'the' and 'jury': 13.0\n",
      "One or both words 'the' and 'cat' are not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "def check_co_occurrence(word1, word2, co_matrix, word_to_index):\n",
    "    \"\"\"\n",
    "    Print the co-occurrence count for a specific word pair.\n",
    "    \"\"\"\n",
    "    if word1 not in word_to_index or word2 not in word_to_index:\n",
    "        print(f\"One or both words '{word1}' and '{word2}' are not in the vocabulary.\")\n",
    "        return\n",
    "\n",
    "    index1 = word_to_index[word1]\n",
    "    index2 = word_to_index[word2]\n",
    "\n",
    "    co_count = co_matrix[index1][index2]\n",
    "    print(f\"Co-occurrence count for '{word1}' and '{word2}': {co_count}\")\n",
    "\n",
    "check_co_occurrence('the', 'jury', co_matrix, word_to_index)\n",
    "check_co_occurrence('the', 'cat', co_matrix, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we reduce the dimensionality of the **co-occurrence matrix** using **Singular Value Decomposition (SVD)**. This technique helps in extracting the most important features from the matrix, making it easier to visualize and interpret the relationships between words.\n",
    "\n",
    "Steps:\n",
    "1. **Singular Value Decomposition (SVD)**: The co-occurrence matrix is decomposed into three matrices: \\( M = U \\cdot \\Sigma \\cdot V^T \\).\n",
    "2. **Dimensionality Reduction**: We retain only the top `k` components (default is 2) from the SVD, effectively reducing the dimensionality of the matrix.\n",
    "3. **Reduced Matrix**: The reduced matrix retains the most important semantic information and can be used for tasks such as **word embedding visualization** or **semantic similarity analysis**.\n",
    "\n",
    "Dimensionality reduction allows us to capture the essence of word relationships in fewer dimensions, making it more efficient to process and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    # Perform SVD\n",
    "    U, S, Vh = np.linalg.svd(M, full_matrices=False)\n",
    "    \n",
    "    # Reduce to k dimensions\n",
    "    M_reduced = U[:, :k] * S[:k]\n",
    "    \n",
    "    return M_reduced\n",
    "\n",
    "reduced_matrix = reduce_to_k_dim(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final part, we visualize the reduced word embeddings in **2D space**. After reducing the dimensionality of the co-occurrence matrix using SVD, we plot the embeddings for each word to observe their relationships in a low-dimensional space.\n",
    "\n",
    "Steps:\n",
    "1. **Plotting the Embeddings**: Each word from the vocabulary is plotted based on its 2D coordinates from the reduced matrix.\n",
    "2. **Visualization**: The scatter plot allows us to see how words cluster together or relate to each other based on co-occurrences in the text. Words with similar contexts should appear closer to each other in the plot.\n",
    "3. **Interpretation**: By looking at the 2D plot, we can analyze word similarities, relationships, and clusters formed by words that often occur together.\n",
    "\n",
    "This visualization is a useful tool for understanding the structure of word embeddings and the relationships between words in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'movie' not found in vocabulary\n",
      "Word 'book' not found in vocabulary\n",
      "Word 'mysterious' not found in vocabulary\n",
      "Word 'story' not found in vocabulary\n",
      "Word 'fascinating' not found in vocabulary\n",
      "Word 'interesting' not found in vocabulary\n",
      "Word 'large' not found in vocabulary\n",
      "Word 'massive' not found in vocabulary\n",
      "Word 'huge' not found in vocabulary\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_embeddings(M_reduced, words):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings in 2D space after dimensionality reduction.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # Create word-to-index mapping\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    # Plot each word that exists in vocabulary\n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            idx = word_to_index[word]\n",
    "            x, y = M_reduced[idx, 0], M_reduced[idx, 1]\n",
    "            plt.scatter(x, y, marker='x', color='blue')\n",
    "            plt.text(x, y, word, fontsize=10, alpha=0.8)\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in vocabulary\")\n",
    "    \n",
    "    plt.title(\"2D Word Embeddings Visualization\", fontsize=14)\n",
    "    plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "    plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "ex_words = ['movie', 'book', 'mysterious', 'story', 'fascinating',\n",
    "         'good', 'interesting', 'large', 'massive', 'huge', 'city', 'police', 'department']\n",
    "\n",
    "plot_embeddings(reduced_matrix, ex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction-Based Word Vectors\n",
    "In this section, we load pre-trained word vectors using the **GloVe** embeddings from the [Stanford NLP paper on GloVe](https://nlp.stanford.edu/pubs/glove.pdf). The pre-trained embeddings provide rich semantic information about words based on large text corpora, allowing us to represent words as vectors that capture their meaning and relationships.\n",
    "\n",
    "Steps:\n",
    "1. **Load GloVe Word Vectors**: We use the GloVe embeddings from the `glove-wiki-gigaword-200` model, which has 200-dimensional vectors for each word.\n",
    "2. **Vocabulary Matching**: We map the words from our corpus to the GloVe vocabulary and extract the corresponding vectors.\n",
    "3. **Dimensionality Reduction**: After obtaining the word vectors, we reduce the dimensionality to visualize the embeddings in 2D space, allowing us to explore word relationships visually.\n",
    "\n",
    "This is an excellent opportunity to explore how pre-trained embeddings like GloVe can be used to improve downstream NLP tasks by providing richer word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Do not edit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m wv_from_bin \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove-wiki-gigaword-200\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Do not edit\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "\n",
    "wv_words = list(wv_from_bin.index_to_key)\n",
    "unique_tokens = set(tokens)\n",
    "word2ind = {}\n",
    "M = []\n",
    "idx = 0\n",
    "print('rendering M based on wv')\n",
    "for w in wv_words:\n",
    "    try:\n",
    "        M.append(wv_from_bin.get_vector(w))\n",
    "        word2ind[w] = idx\n",
    "        idx += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('rendering M based on unique_tokens')\n",
    "for w in unique_tokens:\n",
    "    if w in wv_words:\n",
    "        continue\n",
    "    try:\n",
    "        M.append(wv_from_bin.get_vector(w))\n",
    "        word2ind[w] = idx\n",
    "        idx += 1\n",
    "    except:\n",
    "        pass\n",
    "M = np.stack(M)\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
    "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare plots\n",
    "What stands out when comparing the two different ways of embedding using the given words?\n",
    "\n",
    "(Use this to help answer the question in your assignment document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(M_reduced_normalized, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words with multiple meanings\n",
    "Polysemes and homonyms are words that have more than one meaning (see [this wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms). Your task is to find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings.\n",
    "\n",
    "For example:\n",
    "- \"leaves\" has both the \"go_away\" and \"a_structure_of_a_plant\" meanings in the top 10.\n",
    "- \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\" meanings in the top 10.\n",
    "\n",
    "You will probably need to try several polysemous or homonymic words before you find one.\n",
    "\n",
    "Once you discover a word that fits this criterion, state the word and explain the multiple meanings that occur in the top 10. Reflect on why many of the polysemous or homonymic words you tried didnt work (i.e., why the top-10 most similar words only contain one of the word's meanings).\n",
    "\n",
    "**Note**: Use the `wv_from_bin.most_similar(word)` function to get the top 10 most similar words. This function ranks all other words in the vocabulary based on their cosine similarity to the given word. For further assistance, you can refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms\n",
    "When considering **Cosine Similarity**, it's often useful to think of **Cosine Distance**, which is simply 1 - Cosine Similarity.\n",
    "\n",
    "Find three words \\( (w_1, w_2, w_3) \\) where:\n",
    "- \\(w_1\\) and \\(w_2\\) are **synonyms**.\n",
    "- \\(w_1\\) and \\(w_3\\) are **antonyms**.\n",
    "\n",
    "However, you need to find a case where **Cosine Distance** \\( (w_1, w_3) < \\text{Cosine Distance} (w_1, w_2) \\). In other words, the antonym is **closer** to the word than its synonym in the vector space.\n",
    "\n",
    "For example, \"happy\" (\\(w_1\\)) may be closer to \"sad\" (\\(w_3\\)) than to \"cheerful\" (\\(w_2\\)), which is counter-intuitive since we expect synonyms to be closer than antonyms. \n",
    "\n",
    "Once you find such an example, provide a possible explanation for why this result may have occurred.\n",
    "\n",
    "Use the `wv_from_bin.distance(w1, w2)` function to compute the cosine distance between two words. You can refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies\n",
    "Word vectors have demonstrated the ability to solve analogies based on their learned semantic relationships. For instance, consider the analogy: \n",
    "\n",
    "**\"man : grandfather :: woman : x\"** \n",
    "\n",
    "(Read: man is to grandfather as woman is to x). Using word vectors, we can find the word \\(x\\) that completes the analogy.\n",
    "\n",
    "In the example provided, we use the `most_similar` function from the Gensim library. This function identifies words that are most similar to the words in the positive list and most dissimilar to those in the negative list. For analogy solving, it effectively computes:\n",
    "\n",
    "\\[ \\text{word}(x) = \\text{most similar to} (\\text{woman} + \\text{grandfather} - \\text{man}) \\]\n",
    "\n",
    "The result is the word with the highest cosine similarity to the target vector. You can use this approach to explore various analogies and gain insights into the semantic structure of word vectors.\n",
    "\n",
    "Refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for more details on the `most_similar` function and how word vectors handle analogy-solving tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wv_from_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run this cell to answer the analogy -- man : grandfather :: woman : x\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(wv_from_bin\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrandfather\u001b[39m\u001b[38;5;124m'\u001b[39m], negative\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wv_from_bin' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
    "print(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in word vectors\n",
    "a. **Observation**: Consider the results of a word vector model when queried with analogies involving \"man\", \"woman\", and \"profession\". What are the top results returned by the model for each query?\n",
    "\n",
    "[Your Answer]\n",
    "\n",
    "b. **Analysis**: Do you observe any gender biases in the results? For example, are certain professions more closely associated with \"man\" or \"woman\" based on the word vectors? Discuss how word embeddings might perpetuate societal biases present in the training data.\n",
    "\n",
    "[Your Answer]\n",
    "\n",
    "c. **Reflection**: How might such biases affect the fairness of machine learning models that use word embeddings? Suggest potential strategies for mitigating gender bias in word vector models.\n",
    "\n",
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
    "print()\n",
    "print(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
